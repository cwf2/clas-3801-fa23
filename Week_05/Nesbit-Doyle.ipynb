{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21646abc-30f3-49f7-9237-682a00f67dfe",
   "metadata": {},
   "source": [
    "## Authorship attribution with Burrows' Delta\n",
    "\n",
    "In this exercise we'll put together some of the things we've learned about textual analysis to see whether we can distinguish two authors of mystery and suspense around the turn of the 20th century. \n",
    "\n",
    "On one hand, we have a set of short stories by [E. Nesbit](https://en.wikipedia.org/wiki/E._Nesbit). She's known mostly as a children's writer, but she also wrote horror and mystery stories, and I've selected some of those (from her collection *Grim Tales*). For comparison, I've chosen a selection of short stories by Arthur Conan Doyle. \n",
    "\n",
    "### Preliminaries\n",
    "\n",
    "#### Create a local copy of the corpus\n",
    "Since we're running this in Google Colab, we have to download the corpus separately from the git repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06b1674-5006-4bc4-a02d-81b2c5734172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab only\n",
    "!rm -rf clas-3801 corpus\n",
    "!git clone https://github.com/cwf2/clas-3801-fa23\n",
    "!mv clas-3801-fa23/Week_05/corpus .\n",
    "!rm -rf clas-3801-fa23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0988e51-0877-479c-9692-ded1d152ecc4",
   "metadata": {},
   "source": [
    "#### import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac3cc5-2f20-42a1-b549-9031d0915cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28781fc-754f-4f88-93f1-9df8d716fca2",
   "metadata": {},
   "source": [
    "#### Load the language model\n",
    "\n",
    "We're going to be using SpaCy for tokenization and lemmatization in this case, rather than the builtin Python string manipulation methods. The first step is to load an English language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d913a5-57c2-4759-b7f9-c21cfb80713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a451555a-c758-47f1-ae93-19c947fc8e7f",
   "metadata": {},
   "source": [
    "### Step 1: Read the files\n",
    "\n",
    "We're going to read the files from a local directory, parse them, and organize the tokenized text into a single table with Pandas.\n",
    "\n",
    "#### First, scan the directory for available texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180c93c7-168d-45a5-9d42-4f649025de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all the filenames from the corpus directory\n",
    "dir_texts = 'corpus'\n",
    "files = [f for f in os.listdir(dir_texts) if not f.startswith('.')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bee1319-a8ce-4835-8f16-cc409151921a",
   "metadata": {},
   "source": [
    "#### Loop over all the files and parse them\n",
    "\n",
    "In this `for` loop we:\n",
    "- read the text of the file into a string\n",
    "- process the string with SpaCy to produce a Doc object\n",
    "- extract just the tokens from the doc\n",
    "- store them along with title and author in a dictionary\n",
    "- append the dictionary to a growing list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a42a45-ac68-4fd7-a079-40ddb441d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with an empty list\n",
    "corpus = []\n",
    "\n",
    "# loop over files\n",
    "for i, filename in enumerate(files):\n",
    "    # extract the author and title from the filename\n",
    "    author, title = filename[:-4].split('_', 1)\n",
    "\n",
    "    # add back the directory name to get a path to the file\n",
    "    path = os.path.join('corpus', filename)\n",
    "\n",
    "    # read the file\n",
    "    print(f'[{i+1}/{len(files)}] {path}', end='...')\n",
    "    \n",
    "    with open(path) as f:\n",
    "        fulltext = f.read()\n",
    "\n",
    "        # parse with SpaCy pipeline\n",
    "        doc = nlp(fulltext)\n",
    "        # extract just the tokens\n",
    "        tokens = [tok for tok in doc]\n",
    "        print(len(doc), 'tokens')\n",
    "\n",
    "        # add a new record to the list\n",
    "        corpus.append(dict(\n",
    "            author = author,\n",
    "            title = title,\n",
    "            token = tokens,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ea619b-f7f1-4edb-9c1e-7e0b752b9a4e",
   "metadata": {},
   "source": [
    "#### Convert our data to a Pandas DataFrame\n",
    "\n",
    "Here we turn our list of dictionaries into a DataFrame to make it easer to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645a99f-b180-4d1b-b22f-af0b7e65a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of dictionaries to a Data Frame\n",
    "corpus = pd.DataFrame(corpus)\n",
    "display(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35cddea-4ea8-42a6-ae70-95d0d5ab6e9b",
   "metadata": {},
   "source": [
    "#### Break out the tokens into separate rows\n",
    "\n",
    "Right now, our table has one row per work, and the `token` column contains a list for each row. Next we're going to use the [`explode()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.explode.html) method to break out the tokens into separate rows. For example, each token in \"From the Dead\" will now have its own row, with the `author` and `title` values copied from the original row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3288461c-fdc9-4ac4-956c-88d9b3ebd115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# break out the `token` column\n",
    "tokens = pd.DataFrame(corpus).explode('token', ignore_index=True)\n",
    "display(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f257a7-22e7-466c-861c-b317658e6173",
   "metadata": {},
   "source": [
    "### Step 2: Feature Extraction\n",
    "\n",
    "Right now, the `token` column is displaying in Jupyter as if it just held the text of the tokens, but really this column contains SpaCy Token objects, each containing a bunch of complex linguistic data. I want to extract three attributes from each token that I can use as features:\n",
    "- `text`: a string representing how it looked in the original story\n",
    "- `lemma_`: a string representing its **lemma** or dictionary headword\n",
    "- `pos_`: a string representing its **part of speech**\n",
    "\n",
    "Note the underscores following `lemma_` and `pos_`. If you don't add the underscore, then SpaCy will give you a numeric code for these features instead of a string.\n",
    "\n",
    "I'm assigning the new features to columns called `text`, `lemma` and `pos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f2513-8e62-4909-9bfd-6eaab74f1a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add `text` column\n",
    "tokens['text'] = [tok.text for tok in tokens['token']]\n",
    "\n",
    "# add `lemma` column\n",
    "tokens['lemma'] = [tok.lemma_ for tok in tokens['token']]\n",
    "\n",
    "# add `pos` column\n",
    "tokens['pos'] = [tok.pos_ for tok in tokens['token']]\n",
    "\n",
    "display(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0611ece0-09c7-40b8-9a0b-e700fd3534cb",
   "metadata": {},
   "source": [
    "#### Tidy up the data a little\n",
    "\n",
    "The table now has one row for every token in the corpus, with columns for the text, lemma, and part of speech. But it's clear that some of the things SpaCy considers tokens aren't words, like punctuation marks, numerals, and some kinds of whitespace. \n",
    "\n",
    "Here I'm going to omit rows according to certain criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d59c775-abc1-4d82-bd54-ff75cc615941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only rows where part of speech is not punctuation\n",
    "tokens = tokens.loc[tokens['pos'] != 'PUNCT']\n",
    "\n",
    "# keep only rows where part of speech is not space\n",
    "tokens = tokens.loc[tokens['pos'] != 'SPACE']\n",
    "\n",
    "# keep only rows where part of speech is not proper noun (i.e. personal names)\n",
    "tokens = tokens.loc[tokens['pos'] != 'PROPN']\n",
    "\n",
    "# keep only rows where the token text has at least one letter in it\n",
    "tokens = tokens.loc[tokens['text'].str.contains(r'[A-Za-z]')]\n",
    "\n",
    "display(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48196cba-656f-45d8-8bea-606c85bfaa7b",
   "metadata": {},
   "source": [
    "### Step 3: Feature vectors for each work\n",
    "\n",
    "Now that we have all the features broken out, we can calculate some statistics that will allow us to characterize each work as a bundle of features.\n",
    "\n",
    "#### Part of speech features\n",
    "\n",
    "Let's start by looking at how each text uses different parts of speech. We'll use the `crosstab()` function to get a count for each unique value in `pos`, for each unique combination of `author` and `title`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f6e51-ca18-49f8-8401-a8d607f45cc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_count = pd.crosstab([tokens['author'], tokens['title']], tokens['pos'])\n",
    "display(pos_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ea679-6c92-4ee1-854f-88fd56c6710e",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "\n",
    "This is already interesting, but of course, some of the texts are longer than others, so the raw counts might not be the most helpful way to look at part of speech use. Instead, we'll calculate how many times each part of speech is used per 1000 tokens.\n",
    "\n",
    "By passing the `normalize` argument to `crosstab()` we can ask Pandas to divide each value by the total for its row (i.e., normalize by *index*). Since each row is one work, this tells us the fraction of all tokens in the work represented by each column. Then we multiply by 1000 just to make the numbers easier to read. We've now converted counts to frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ec33f-83ae-4a6f-b0e2-80701698833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_freq = pd.crosstab([tokens['author'], tokens['title']], tokens['pos'], normalize=\"index\")*1000\n",
    "display(pos_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0702a5-89b2-4323-8f04-fc36f290fdb7",
   "metadata": {},
   "source": [
    "#### Plotting\n",
    "\n",
    "Let's create a simple plot. We'll use the x-axis to represent the frequency of one part of speech, and the y-axis to represent a different frequency. Each text will be located somewhere in the cartesian space defined by these two features.\n",
    "\n",
    "I'm using Pyplot to create my graph. First I have to create a [Figure and Axes](https://matplotlib.org/stable/users/explain/axes/axes_intro.html), and then I can use the Axes to add elements to the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f58872-3f3f-47ad-aee5-089644ac2589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the features I want to look at\n",
    "feat_x = 'NOUN'\n",
    "feat_y = 'VERB'\n",
    "\n",
    "# extract the list of author names from the table\n",
    "#   - this is for labelling the points\n",
    "authors = pos_freq.index.get_level_values(0)\n",
    "\n",
    "# instantiate a new figure and axes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# add a new series to the plot for each author\n",
    "for label, group in pos_freq.groupby(authors):\n",
    "    ax.plot(group[feat_x], group[feat_y], marker='o', ls='', label=label)\n",
    "\n",
    "# label the plot axes\n",
    "ax.set_xlabel(feat_x)\n",
    "ax.set_ylabel(feat_y)\n",
    "\n",
    "# create a legend\n",
    "ax.legend()\n",
    "\n",
    "# display results\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1c7c13-c723-4ef0-a59c-4f4eba066a39",
   "metadata": {},
   "source": [
    "##### Try it out:\n",
    "\n",
    "🤔 Try changing the part of speech values above and re-running the plot. Are some parts of speech better than others and differentiating the two authors?\n",
    "\n",
    "### Lemma-based features\n",
    "\n",
    "Let's try working with a larger feature set. This time, we'll use lemmas instead of part of speech tags. There are only 14 parts of speech in our table, but there are a lot of lemmas in this corpus.\n",
    "\n",
    "Let's calculate the total number of occurrences for each lemma. This time, I'll use the `groupby()` method to split the table by lemmas. Then I'll **aggregate** the groups using the `agg()` method. This turns each group into a single value, based on whatever **aggregation function** I choose. If I wanted, I could aggregate different columns using different methods. \n",
    "\n",
    "For now, I'm just going to aggregate the `token` column and I'm going to use a simple count. The result will be a table with one row per unique lemma, and a column containing the count of the number of tokens that have that lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f098e5f-10ed-4eaf-adda-9b447c2f7697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count tokens to produce a new table\n",
    "token_count = tokens.groupby('lemma').agg(count=('token', 'count'))\n",
    "\n",
    "# sort by count, decreasing order\n",
    "token_count = token_count.sort_values('count', ascending=False)\n",
    "\n",
    "display(token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4f3783-858c-457c-8e2a-047be3f39b46",
   "metadata": {},
   "source": [
    "So there are 6345 unique lemmas in this corpus. How many are *hapax legomena*, occurring only once? I can use the fact that the `sum()` function counts `True` as 1 and `False` as 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec0733b-01b6-433c-8899-e3e9413fbb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(token_count['count'] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc0ffd8-1248-4420-9844-98a25ac38cf6",
   "metadata": {},
   "source": [
    "Wow! So out of 6345 lemmas, more than a third are hapax legomena. This graph has a long tail! Does it follow Zipf's law?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef62628-d4a1-4f3a-aa98-e1d0a745a62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count.plot(loglog=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a934a-2f0e-4335-80f1-fe61a64a8e77",
   "metadata": {},
   "source": [
    "#### Feature selection\n",
    "\n",
    "For authorship attribution, we want to work with just the function words. Generally this means working with the *n* **most frequent words** (MFW). The value of *n* is something that we might need to tune through trial and error. Let's start with the top 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a3e7c7-8881-4350-99b4-30e20fe6d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count.iloc[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9badb1a-fbb1-4487-97bf-26d66955b609",
   "metadata": {},
   "source": [
    "We'll take the row names from the top *n* lines and use that as our \"keep\" list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1cf514-75e5-4d87-97da-fabca775fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfw = token_count.iloc[:30].index.values\n",
    "print(mfw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bf7292-45d9-4e3f-94cb-560449921dd8",
   "metadata": {},
   "source": [
    "I'll create a list of boolean values marking which rows in the token table match words in my feature set. I can use this as a mask to select just those rows in future operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e926b16d-dc5b-4d50-a93a-a6b297ea30c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = tokens.lemma.isin(mfw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f9d6e8-f9c8-446d-aa20-50cba318a718",
   "metadata": {},
   "source": [
    "#### Calculate the feature vectors\n",
    "\n",
    "Here we'll use `crosstab()` again, just as with the part of speech features. The only difference is I'm masking the token table so that I'm only working on rows that match my feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6650a159-9f6e-4aa7-9204-376b05141a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_count = pd.crosstab([tokens.loc[selected, 'author'],tokens.loc[selected, 'title']], tokens.loc[selected, 'lemma'])\n",
    "display(lemma_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962324a2-e647-4af9-85de-5dd1019d387a",
   "metadata": {},
   "source": [
    "#### Normalize\n",
    "\n",
    "As before, we want to switch from raw counts per text to frequencies, so that shorter and longer texts become more comparable. The only problem here is that we've left out most of the tokens, so we can't just divide by the row totals anymore. Instead, we'll have to go back and calculate how many tokens are in each text in a separate table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83896be0-c920-41f4-9e64-906462d42f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a table of lemma counts\n",
    "n_lemmas = tokens.groupby('title').agg(\n",
    "    n=('lemma', 'count'),\n",
    "    \n",
    ")\n",
    "\n",
    "# normalize feature vectors by lemma counts per text\n",
    "lemma_freq = lemma_count.div(n_lemmas.n, axis=0) * 1000\n",
    "display(lemma_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc5daf-09cc-4d7b-9ede-a2b4acff5b1c",
   "metadata": {},
   "source": [
    "### Part 4: Zeta scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d2d38-7983-4e9a-8827-631579572b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_z = lemma_freq.sub(lemma_freq.mean(), axis=1).div(lemma_freq.std(), axis=1)\n",
    "display(lemma_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e901cb95-7811-422d-8a8c-48e4d4c6ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_x = 'and'\n",
    "feat_y = 'but'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for label, group in lemma_z.groupby(authors):\n",
    "    ax.plot(group[feat_x], group[feat_y], marker='o', ls='', label=label)\n",
    "ax.set_xlabel(feat_x)\n",
    "ax.set_ylabel(feat_y)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b974788a-56e8-4f43-9171-4d3e1cd6c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model = PCA(n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4903075a-1949-4f26-8569-25feb677264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = pd.DataFrame(\n",
    "    pca_model.fit_transform(lemma_z),\n",
    "    index = lemma_z.index,\n",
    "    columns = ['PC1', 'PC2', 'PC3'],\n",
    ")\n",
    "display(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6156e90c-0c4d-4626-8267-918e26d3ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_x = 'PC1'\n",
    "feat_y = 'PC2'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for label, group in pca.groupby(authors):\n",
    "    ax.plot(group[feat_x], group[feat_y], marker='o', ls='', label=label)\n",
    "ax.set_xlabel(feat_x)\n",
    "ax.set_ylabel(feat_y)\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
